# Micrograd
Credits to Andrej Karpathy
The creation of 'Micrograd' gave me a grasp of the inner working of libraries used for deep learning and neural network building. Micrograd can perform complex calculations needed to obtain a neural network output and then update the gradients of the weights and biases by using backpropagration. All these calculations will be done with the goal of reducing the loss, obtained by the Sum of the Squared Errors, and therefore obatining more accurate predictions. The code can be accessed by clicking over the 'project details' button below.
